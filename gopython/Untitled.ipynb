{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05246dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/22 10:39:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/22 10:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# import pymysql\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JDBC Connection\").config(\"spark.driver.extraClassPath\", \"/Users/qingyanchen/Downloads/mysql-connector-java-8.0.25.jar\").getOrCreate()\n",
    "\n",
    "# 设置 JDBC 配置信息\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/testdata\"\n",
    "jdbc_driver = \"com.mysql.jdbc.Driver\"\n",
    "jdbc_username = \"root\"\n",
    "jdbc_password = \"123456\"\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"mytable\", properties={\"driver\": jdbc_driver, \"user\": jdbc_username, \"password\": jdbc_password})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc340e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|count|\n",
      "+---+----+-----+\n",
      "|  1|John|    1|\n",
      "|  2|Jane|    1|\n",
      "|  3|Mary|    1|\n",
      "|  4| Tom|    1|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f39deda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|count|\n",
      "+---+----+-----+\n",
      "|  1|John|    1|\n",
      "|  2|Jane|    1|\n",
      "|  3|Mary|    1|\n",
      "|  4| Tom|    1|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的模块\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# 创建一个DataFrame\n",
    "df = spark.createDataFrame([(1, \"John\", 25), (2, \"Jane\", 30), (3, \"Mary\", 35), (4, \"Tom\", 40)], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# 对第一列进行分组，然后对第二列进行分组\n",
    "result = df.groupBy(\"id\", \"name\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# 打印结果\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e814e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|          timestamp|     ip_address|\n",
      "+-------------------+---------------+\n",
      "|2023-02-25 09:29:52| 118.144.72.252|\n",
      "|2023-01-11 20:24:30| 123.100.190.59|\n",
      "|2022-07-02 13:49:32|123.199.128.174|\n",
      "|2023-01-05 13:06:53| 210.185.192.55|\n",
      "|2022-08-31 12:32:08|  124.68.89.156|\n",
      "|2022-08-14 20:50:43|  119.27.64.233|\n",
      "|2022-07-24 11:33:44|   119.48.4.102|\n",
      "|2022-08-14 18:56:40| 117.120.64.237|\n",
      "|2023-02-11 04:47:49|123.101.118.142|\n",
      "|2022-07-25 22:23:42| 218.192.230.43|\n",
      "|2023-03-19 20:24:51|  119.96.54.177|\n",
      "|2023-04-17 23:44:47| 116.214.64.170|\n",
      "|2022-06-22 09:20:29| 202.127.16.140|\n",
      "|2022-12-30 07:08:46| 210.16.128.254|\n",
      "|2022-08-29 12:52:37|   219.82.0.175|\n",
      "|2022-12-08 16:05:38| 210.51.207.198|\n",
      "|2023-03-21 20:26:57|  203.91.32.151|\n",
      "|2023-01-19 10:57:05| 202.125.176.44|\n",
      "|2022-10-01 15:17:19|  221.129.67.57|\n",
      "|2023-03-08 05:27:33|  203.88.32.137|\n",
      "+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#读取日志文件\n",
    "web_logs = spark.read.json('web_log1.json')\n",
    "#选择所要的字段\n",
    "time_ip=web_logs.select(\"timestamp\",\"ip_address\")\n",
    "time_ip.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce9534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the JDBC URL for the MySQL database\n",
    "url = \"jdbc:mysql://localhost:3306/testdata\"\n",
    "\n",
    "# Define the properties for the JDBC driver\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"123456\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Write the data to the MySQL database\n",
    "time_ip.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .jdbc(url=url, table=\"timeip\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c68645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|count|\n",
      "+---+----+-----+\n",
      "|  1|John|    1|\n",
      "|  2|Jane|    1|\n",
      "|  3|Mary|    1|\n",
      "|  4| Tom|    1|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(url=jdbc_url, table=\"mytable\", properties={\"driver\": jdbc_driver, \"user\": jdbc_username, \"password\": jdbc_password})\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41952cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "| timestamp|response_content_length|\n",
      "+----------+-----------------------+\n",
      "|2022-10-05|              152077609|\n",
      "|2023-01-21|              120910569|\n",
      "|2023-05-01|              129253547|\n",
      "|2023-04-17|              128518167|\n",
      "|2023-04-21|              131877678|\n",
      "|2023-04-28|              137402571|\n",
      "|2022-10-07|              146889317|\n",
      "|2023-02-10|              129507120|\n",
      "|2022-05-17|              137581041|\n",
      "|2023-05-04|              125880810|\n",
      "|2023-04-26|              130263786|\n",
      "|2023-03-10|              117255798|\n",
      "|2023-03-11|              108219484|\n",
      "|2022-07-04|              136946829|\n",
      "|2023-02-23|              137311455|\n",
      "|2022-07-08|              120125752|\n",
      "|2022-07-30|              128714011|\n",
      "|2023-03-17|              123034522|\n",
      "|2022-09-03|              127935542|\n",
      "|2022-07-23|              149043693|\n",
      "+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "# import pymysql\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JDBC Connection\").config(\"spark.driver.extraClassPath\", \"/Users/qingyanchen/Downloads/mysql-connector-java-8.0.25.jar\").getOrCreate()\n",
    "\n",
    "web_logs = spark.read.json('web_log1.json')\n",
    "web_logs2=web_logs.select(\"timestamp\",\"response_content_length\")\n",
    "split_col = functions.split(web_logs2['timestamp'], ' ')\n",
    "web_logs2=web_logs2.withColumn('timestamp', split_col.getItem(0))\n",
    "\n",
    "web_logs2=web_logs2.withColumn('response_content_length',functions.split(web_logs2['response_content_length'],'bytes').getItem(0))\n",
    "\n",
    "web_logs2=web_logs2.withColumn('response_content_length',col('response_content_length').cast(\"int\"))\n",
    "web_logs2=web_logs2.groupby(\"timestamp\").sum(\"response_content_length\").alias(\"response_content_length\")\n",
    "web_logs2=web_logs2.select(web_logs2[\"timestamp\"],web_logs2[\"sum(response_content_length)\"].alias(\"response_content_length\"))\n",
    "url = \"jdbc:mysql://localhost:3306/testdata\"\n",
    "\n",
    "# Define the properties for the JDBC driver\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"123456\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "web_logs2.write.mode(\"append\").jdbc(url=url, table=\"time_sum\", properties=properties)\n",
    "web_logs2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "937daacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+----+-----+---+--------+\n",
      "|          timestamp|response_content_length|year|month|day|    time|\n",
      "+-------------------+-----------------------+----+-----+---+--------+\n",
      "|2020-05-01 00:14:45|                  67616|2020|   05| 05|00:14:45|\n",
      "|2020-05-01 00:17:59|                 884584|2020|   05| 05|00:17:59|\n",
      "|2020-05-01 00:22:54|                 495177|2020|   05| 05|00:22:54|\n",
      "|2020-05-01 00:28:33|                 217380|2020|   05| 05|00:28:33|\n",
      "|2020-05-01 00:59:05|                  24874|2020|   05| 05|00:59:05|\n",
      "|2020-05-01 01:05:52|                 769806|2020|   05| 05|01:05:52|\n",
      "|2020-05-01 01:23:38|                 190584|2020|   05| 05|01:23:38|\n",
      "|2020-05-01 01:24:12|                  67674|2020|   05| 05|01:24:12|\n",
      "|2020-05-01 02:09:27|                 412041|2020|   05| 05|02:09:27|\n",
      "|2020-05-01 02:32:30|                 735547|2020|   05| 05|02:32:30|\n",
      "|2020-05-01 02:52:48|                 192387|2020|   05| 05|02:52:48|\n",
      "|2020-05-01 03:09:08|                 223918|2020|   05| 05|03:09:08|\n",
      "|2020-05-01 03:20:58|                 310211|2020|   05| 05|03:20:58|\n",
      "|2020-05-01 03:58:49|                 199162|2020|   05| 05|03:58:49|\n",
      "|2020-05-01 04:30:40|                 689227|2020|   05| 05|04:30:40|\n",
      "|2020-05-01 04:45:02|                 456764|2020|   05| 05|04:45:02|\n",
      "|2020-05-01 05:03:47|                 651488|2020|   05| 05|05:03:47|\n",
      "|2020-05-01 05:11:37|                 980357|2020|   05| 05|05:11:37|\n",
      "|2020-05-01 05:18:40|                 411264|2020|   05| 05|05:18:40|\n",
      "|2020-05-01 05:21:37|                  27463|2020|   05| 05|05:21:37|\n",
      "+-------------------+-----------------------+----+-----+---+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----+\n",
      "|timestamp|count|\n",
      "+---------+-----+\n",
      "|  2020-05| 2878|\n",
      "|  2020-06| 2821|\n",
      "|  2020-07| 3006|\n",
      "|  2020-08| 2932|\n",
      "|  2020-09| 2841|\n",
      "|  2020-10| 2937|\n",
      "|  2020-11| 2850|\n",
      "|  2020-12| 2842|\n",
      "|  2021-01| 2967|\n",
      "|  2021-02| 2707|\n",
      "|  2021-03| 2986|\n",
      "|  2021-04| 2884|\n",
      "|  2021-05| 2895|\n",
      "|  2021-06| 2848|\n",
      "|  2021-07| 2911|\n",
      "|  2021-08| 3015|\n",
      "|  2021-09| 2834|\n",
      "|  2021-10| 2978|\n",
      "|  2021-11| 2757|\n",
      "|  2021-12| 2933|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+\n",
      "|          timestamp|response_content_length|\n",
      "+-------------------+-----------------------+\n",
      "|2023-04-30 19:21:04|                 757957|\n",
      "|2022-10-05 22:43:32|                 980446|\n",
      "|2022-08-15 08:50:38|                 376207|\n",
      "|2022-12-29 18:30:47|                 487427|\n",
      "|2023-03-15 12:56:29|                 614381|\n",
      "|2022-05-25 04:03:05|                 326598|\n",
      "|2022-09-08 17:32:54|                 225532|\n",
      "|2022-08-26 17:18:39|                 531206|\n",
      "|2022-05-28 12:27:37|                  93236|\n",
      "|2023-04-27 13:00:43|                 580012|\n",
      "|2022-05-27 01:35:19|                 616888|\n",
      "|2023-01-13 01:53:44|                 696999|\n",
      "|2022-10-21 09:43:01|                 225673|\n",
      "|2022-08-09 19:45:15|                 940934|\n",
      "|2022-06-16 07:05:28|                 564542|\n",
      "|2023-04-28 18:43:38|                 305806|\n",
      "|2022-10-10 13:04:59|                 579152|\n",
      "|2023-01-03 15:11:24|                  93368|\n",
      "|2022-08-09 08:13:30|                 316697|\n",
      "|2023-04-14 04:23:39|                 447824|\n",
      "+-------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "# import pymysql\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JDBC Connection\").config(\"spark.driver.extraClassPath\", \"/Users/qingyanchen/Downloads/mysql-connector-java-8.0.25.jar\").getOrCreate()\n",
    "\n",
    "web_logs = spark.read.json('web_log1.json')\n",
    "web_logs2=web_logs.select(\"timestamp\",\"response_content_length\")\n",
    "split_col = functions.split(web_logs2['timestamp'], ' ')\n",
    "web_logs2=web_logs2.withColumn('year', functions.split(web_logs2['timestamp'], '-').getItem(0))\n",
    "web_logs2=web_logs2.withColumn('month', functions.split(web_logs2['timestamp'], '-').getItem(1))\n",
    "web_logs2=web_logs2.withColumn('day', functions.split(web_logs2['timestamp'], '-').getItem(1))\n",
    "\n",
    "web_logs2=web_logs2.withColumn('time', functions.split(web_logs2['timestamp'], ' ').getItem(1))\n",
    "web_logs2=web_logs2.withColumn('response_content_length',functions.split(web_logs2['response_content_length'],'bytes').getItem(0)).orderBy(\"timestamp\")\n",
    "web_logs2.show()\n",
    "\n",
    "\n",
    "web_logs3=web_logs2.withColumn('timestamp',functions.concat_ws('-',web_logs2['year'],web_logs2['month'])).select(\"timestamp\",\"response_content_length\")\n",
    "web_logs3=web_logs3.withColumn('response_content_length',col('response_content_length').cast(\"int\"))\n",
    "web_logs3=web_logs3.groupby(\"timestamp\").count().alias(\"count\").orderBy(\"timestamp\")\n",
    "# web_logs3.write.mode(\"append\").jdbc(url=url, table=\"time_sum\", properties=properties)\n",
    "# web_logs3=web_logs3.select(web_logs3[\"timestamp\"],web_logs3[\"sum(response_content_length)\"].alias(\"response_content_length\")).orderBy(\"timestamp\")\n",
    "web_logs3.show()\n",
    "web_logs2.write.mode(\"append\").jdbc(url=url, table=\"time_count\", properties=properties)\n",
    "\n",
    "\n",
    "web_logs2=web_logs2.withColumn('response_content_length',col('response_content_length').cast(\"int\"))\n",
    "web_logs2=web_logs2.groupby(\"timestamp\").sum(\"response_content_length\").alias(\"response_content_length\")\n",
    "web_logs2=web_logs2.select(web_logs2[\"timestamp\"],web_logs2[\"sum(response_content_length)\"].alias(\"response_content_length\"))\n",
    "# web_logs3.write.mode(\"append\").jdbc(url=url, table=\"time_count\", properties=properties)\n",
    "web_logs2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618341ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1e4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb9694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d53850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb47cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|timestamp|response_content_length|\n",
      "+---------+-----------------------+\n",
      "| 09:29:52|            305864bytes|\n",
      "| 20:24:30|             50292bytes|\n",
      "| 13:49:32|            900609bytes|\n",
      "| 13:06:53|            542058bytes|\n",
      "| 12:32:08|             32101bytes|\n",
      "| 20:50:43|             17540bytes|\n",
      "| 11:33:44|             54501bytes|\n",
      "| 18:56:40|            870018bytes|\n",
      "| 04:47:49|            456133bytes|\n",
      "| 22:23:42|             42329bytes|\n",
      "| 20:24:51|            855881bytes|\n",
      "| 23:44:47|            813620bytes|\n",
      "| 09:20:29|             17861bytes|\n",
      "| 07:08:46|            623334bytes|\n",
      "| 12:52:37|            395794bytes|\n",
      "| 16:05:38|            119055bytes|\n",
      "| 20:26:57|            252035bytes|\n",
      "| 10:57:05|            149316bytes|\n",
      "| 15:17:19|            128064bytes|\n",
      "| 05:27:33|            662544bytes|\n",
      "+---------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----------------------+\n",
      "|timestamp|response_content_length|\n",
      "+---------+-----------------------+\n",
      "| 09:29:52|                 305864|\n",
      "| 20:24:30|                  50292|\n",
      "| 13:49:32|                 900609|\n",
      "| 13:06:53|                 542058|\n",
      "| 12:32:08|                  32101|\n",
      "| 20:50:43|                  17540|\n",
      "| 11:33:44|                  54501|\n",
      "| 18:56:40|                 870018|\n",
      "| 04:47:49|                 456133|\n",
      "| 22:23:42|                  42329|\n",
      "| 20:24:51|                 855881|\n",
      "| 23:44:47|                 813620|\n",
      "| 09:20:29|                  17861|\n",
      "| 07:08:46|                 623334|\n",
      "| 12:52:37|                 395794|\n",
      "| 16:05:38|                 119055|\n",
      "| 20:26:57|                 252035|\n",
      "| 10:57:05|                 149316|\n",
      "| 15:17:19|                 128064|\n",
      "| 05:27:33|                 662544|\n",
      "+---------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+----------------------------+\n",
      "|timestamp|sum(response_content_length)|\n",
      "+---------+----------------------------+\n",
      "| 07:33:20|                       28239|\n",
      "| 22:15:57|                      672864|\n",
      "| 23:07:29|                      664203|\n",
      "| 01:40:01|                      900947|\n",
      "| 06:47:42|                     1575983|\n",
      "| 03:42:56|                     1254715|\n",
      "| 19:32:33|                      565965|\n",
      "| 01:59:21|                      985825|\n",
      "| 20:34:08|                      644065|\n",
      "| 07:11:59|                      621609|\n",
      "| 03:33:56|                      899566|\n",
      "| 21:31:51|                     1225570|\n",
      "| 10:54:22|                     1565073|\n",
      "| 07:12:18|                     1033011|\n",
      "| 11:46:54|                      771603|\n",
      "| 03:00:01|                      299398|\n",
      "| 15:50:05|                     1393817|\n",
      "| 20:58:13|                      773224|\n",
      "| 18:33:38|                      125487|\n",
      "| 22:33:14|                      161492|\n",
      "+---------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "# 读取JSON格式的Web日志文件\n",
    "# \n",
    "web_logs = spark.read.json('web_log1.json')\n",
    "web_logs2=web_logs.select(\"timestamp\",\"response_content_length\")\n",
    "split_col = functions.split(web_logs2['timestamp'], ' ')\n",
    "web_logs2=web_logs2.withColumn('timestamp', split_col.getItem(1))\n",
    "web_logs2.show()\n",
    "web_logs2=web_logs2.withColumn('response_content_length',functions.split(web_logs2['response_content_length'],'bytes').getItem(0))\n",
    "web_logs2.show()\n",
    "from pyspark.sql.functions import col\n",
    "# df = df.withColumn(\"age\", col(\"age\").cast(\"float\"))\n",
    "web_logs2=web_logs2.withColumn('response_content_length',col('response_content_length').cast(\"int\"))\n",
    "web_logs2=web_logs2.groupby(\"timestamp\").sum(\"response_content_length\")\n",
    "# select count() from mydata.mytable\n",
    "web_logs2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26ba999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|timestamp|response_content_length|\n",
      "+---------+-----------------------+\n",
      "| 09:29:52|            305864bytes|\n",
      "| 20:24:30|             50292bytes|\n",
      "| 13:49:32|            900609bytes|\n",
      "| 13:06:53|            542058bytes|\n",
      "| 12:32:08|             32101bytes|\n",
      "| 20:50:43|             17540bytes|\n",
      "| 11:33:44|             54501bytes|\n",
      "| 18:56:40|            870018bytes|\n",
      "| 04:47:49|            456133bytes|\n",
      "| 22:23:42|             42329bytes|\n",
      "| 20:24:51|            855881bytes|\n",
      "| 23:44:47|            813620bytes|\n",
      "| 09:20:29|             17861bytes|\n",
      "| 07:08:46|            623334bytes|\n",
      "| 12:52:37|            395794bytes|\n",
      "| 16:05:38|            119055bytes|\n",
      "| 20:26:57|            252035bytes|\n",
      "| 10:57:05|            149316bytes|\n",
      "| 15:17:19|            128064bytes|\n",
      "| 05:27:33|            662544bytes|\n",
      "+---------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----------------------+\n",
      "|timestamp|response_content_length|\n",
      "+---------+-----------------------+\n",
      "| 09:29:52|                 305864|\n",
      "| 20:24:30|                  50292|\n",
      "| 13:49:32|                 900609|\n",
      "| 13:06:53|                 542058|\n",
      "| 12:32:08|                  32101|\n",
      "| 20:50:43|                  17540|\n",
      "| 11:33:44|                  54501|\n",
      "| 18:56:40|                 870018|\n",
      "| 04:47:49|                 456133|\n",
      "| 22:23:42|                  42329|\n",
      "| 20:24:51|                 855881|\n",
      "| 23:44:47|                 813620|\n",
      "| 09:20:29|                  17861|\n",
      "| 07:08:46|                 623334|\n",
      "| 12:52:37|                 395794|\n",
      "| 16:05:38|                 119055|\n",
      "| 20:26:57|                 252035|\n",
      "| 10:57:05|                 149316|\n",
      "| 15:17:19|                 128064|\n",
      "| 05:27:33|                 662544|\n",
      "+---------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+----------------------------+\n",
      "|timestamp|sum(response_content_length)|\n",
      "+---------+----------------------------+\n",
      "| 07:33:20|                       28239|\n",
      "| 22:15:57|                      672864|\n",
      "| 23:07:29|                      664203|\n",
      "| 01:40:01|                      900947|\n",
      "| 06:47:42|                     1575983|\n",
      "| 03:42:56|                     1254715|\n",
      "| 19:32:33|                      565965|\n",
      "| 01:59:21|                      985825|\n",
      "| 20:34:08|                      644065|\n",
      "| 07:11:59|                      621609|\n",
      "| 03:33:56|                      899566|\n",
      "| 21:31:51|                     1225570|\n",
      "| 10:54:22|                     1565073|\n",
      "| 07:12:18|                     1033011|\n",
      "| 11:46:54|                      771603|\n",
      "| 03:00:01|                      299398|\n",
      "| 15:50:05|                     1393817|\n",
      "| 20:58:13|                      773224|\n",
      "| 18:33:38|                      125487|\n",
      "| 22:33:14|                      161492|\n",
      "+---------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "# 读取JSON格式的Web日志文件\n",
    "# \n",
    "web_logs = spark.read.json('web_log1.json')\n",
    "web_logs2=web_logs.select(\"timestamp\",\"response_content_length\")\n",
    "split_col = functions.split(web_logs2['timestamp'], ' ')\n",
    "web_logs2=web_logs2.withColumn('timestamp', split_col.getItem(1))\n",
    "web_logs2.show()\n",
    "web_logs2=web_logs2.withColumn('response_content_length',functions.split(web_logs2['response_content_length'],'bytes').getItem(0))\n",
    "web_logs2.show()\n",
    "from pyspark.sql.functions import col\n",
    "# df = df.withColumn(\"age\", col(\"age\").cast(\"float\"))\n",
    "web_logs2=web_logs2.withColumn('response_content_length',col('response_content_length').cast(\"int\"))\n",
    "web_logs2=web_logs2.groupby(\"timestamp\").sum(\"response_content_length\")\n",
    "# select count() from mydata.mytable\n",
    "web_logs2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "# 读取JSON格式的Web日志文件\n",
    "# \n",
    "web_logs = spark.read.json('web_log1.json')\n",
    "web_logs2=web_logs.select(\"timestamp\",\"response_content_length\")\n",
    "split_col = functions.split(web_logs2['timestamp'], ' ')\n",
    "web_logs2=web_logs2.withColumn('timestamp', split_col.getItem(1))\n",
    "web_logs2.show()\n",
    "web_logs2=web_logs2.withColumn('response_content_length',functions.split(web_logs2['response_content_length'],'bytes').getItem(0))\n",
    "web_logs2.show()\n",
    "from pyspark.sql.functions import col\n",
    "# df = df.withColumn(\"age\", col(\"age\").cast(\"float\"))\n",
    "web_logs2=web_logs2.withColumn('response_content_length',col('response_content_length').cast(\"int\"))\n",
    "web_logs2=web_logs2.groupby(\"timestamp\").sum(\"response_content_length\")\n",
    "# select count() from mydata.mytable\n",
    "web_logs2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f38494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "| timestamp|load_type|count|\n",
      "+----------+---------+-----+\n",
      "|2022-07-10|     党政|   27|\n",
      "|2022-05-01|     观点|   24|\n",
      "|2022-05-17|     地方|  173|\n",
      "|2023-02-21|     观点|   27|\n",
      "|2022-08-30|     党政|   36|\n",
      "|2022-10-08|     党政|   25|\n",
      "|2022-10-12|     要闻|   30|\n",
      "|2022-10-28|     要闻|   43|\n",
      "|2023-01-19|     地方|  183|\n",
      "|2023-03-16|     观点|   29|\n",
      "|2022-11-30|     要闻|   46|\n",
      "|2023-04-20|     观点|   32|\n",
      "|2023-05-03|     党政|   34|\n",
      "|2023-04-10|     党政|   26|\n",
      "|2022-10-29|     观点|   24|\n",
      "|2022-10-10|     要闻|   43|\n",
      "|2022-05-26|     观点|   31|\n",
      "|2022-12-28|     党政|   34|\n",
      "|2022-07-13|     党政|   20|\n",
      "|2022-05-05|     要闻|   50|\n",
      "+----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import col,count\n",
    "from pyspark.sql import SparkSession\n",
    "# import pymysql\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JDBC Connection\").config(\"spark.driver.extraClassPath\", \"/Users/qingyanchen/Downloads/mysql-connector-java-8.0.25.jar\").getOrCreate()\n",
    "\n",
    "web_logs = spark.read.json('web_log1.json')\n",
    "load_type=web_logs.select(\"timestamp\",\"load_type\")\n",
    "#生成对类型切割的函数\n",
    "split_type=functions.split(load_type[\"load_type\"],'/')\n",
    "#对类型列进行切割\n",
    "load_type2=load_type.withColumn('load_type',split_type.getItem(0))\n",
    "#生成对时间戳切割的函数\n",
    "load_type3 = functions.split(load_type2['timestamp'], ' ')\n",
    "#对时间戳列进行切割\n",
    "load_type4=load_type2.withColumn('timestamp', load_type3.getItem(0))\n",
    "load_type4=load_type4.groupBy(\"timestamp\",\"load_type\").agg(count(\"*\").alias(\"count\"))\n",
    "# load_type4=load_type4\n",
    "load_type4.show()\n",
    "# Define the JDBC URL for the MySQL database\n",
    "url = \"jdbc:mysql://localhost:3306/testdata\"\n",
    "# Define the properties for the JDBC driver\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"123456\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "# load_type4.write.mode(\"append\").jdbc(url=url, table=\"type_count\", properties=properties)\n",
    "load_type4.write.mode(\"append\").jdbc(url=url, table=\"type_count\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0e109fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "| timestamp|load_type|count|\n",
      "+----------+---------+-----+\n",
      "|2023-05-03|     党政|   30|\n",
      "|2023-05-02|     观点|   26|\n",
      "|2023-05-08|     观点|   18|\n",
      "|2023-05-01|     观点|   16|\n",
      "|2023-05-09|     观点|   20|\n",
      "|2023-05-07|     党政|   16|\n",
      "|2023-05-05|     观点|   17|\n",
      "|2023-05-05|     地方|  151|\n",
      "|2023-05-02|     党政|   35|\n",
      "|2023-05-09|     要闻|   31|\n",
      "|2023-05-01|     地方|  152|\n",
      "|2023-05-05|     党政|   35|\n",
      "|2023-05-04|     地方|  150|\n",
      "|2023-05-06|     地方|  159|\n",
      "|2023-05-08|     地方|  151|\n",
      "|2023-05-03|     观点|   20|\n",
      "|2023-05-06|     观点|   15|\n",
      "|2023-05-03|     要闻|   34|\n",
      "|2023-05-08|     党政|   25|\n",
      "|2023-05-09|     党政|   31|\n",
      "+----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the JDBC URL for the MySQL database\n",
    "url = \"jdbc:mysql://localhost:3306/testdata\"\n",
    "\n",
    "# Define the properties for the JDBC driver\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"123456\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "load_type4.write.mode(\"append\").jdbc(url=url, table=\"type_count\", properties=properties)\n",
    "load_type4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3e4b4bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m split_col \u001b[38;5;241m=\u001b[39m functions\u001b[38;5;241m.\u001b[39msplit(web_logs2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m web_logs2\u001b[38;5;241m=\u001b[39mweb_logs2\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, split_col\u001b[38;5;241m.\u001b[39mgetItem(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m web_logs2\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\u001b[43mcount\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "# split_col = functions.split(web_logs['timestamp'], ' ')\n",
    "# web_logs.withColumn('timestamp', split_col.getItem(1)).show(2)\n",
    "# df = web_logs.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "start_time = \"2023-05-04 00:00:00\"\n",
    "end_time = \"2023-05-05 23:59:59\"\n",
    "# web_logs.agg(count(\"*\").alias(\"count\")).show()\n",
    "web_logs2=web_logs.filter((col(\"timestamp\") >= start_time) & (col(\"timestamp\") <= end_time))\n",
    "split_col = functions.split(web_logs2['timestamp'], ':')\n",
    "web_logs2=web_logs2.withColumn('timestamp', split_col.getItem(0))\n",
    "web_logs2.groupBy(\"timestamp\").agg(count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10cb46f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  361|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = \"2023-05-03 00:00:00\"\n",
    "end_time = \"2023-05-04 23:59:59\"\n",
    "# web_logs.agg(count(\"*\").alias(\"count\")).show()\n",
    "web_logs.filter((col(\"timestamp\") >= start_time) & (col(\"timestamp\") <= end_time)).agg(count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8235e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+\n",
      "| timestamp|load_type|count(1)|\n",
      "+----------+---------+--------+\n",
      "|2015-03-18|   内蒙古|       1|\n",
      "|2021-05-25|     浙江|       3|\n",
      "|2004-09-12|     贵州|       1|\n",
      "|2014-01-25|     云南|       1|\n",
      "|2006-03-08|     江苏|       3|\n",
      "|2022-12-29|     宁夏|       4|\n",
      "|2013-04-14|     陕西|       1|\n",
      "|2002-11-06|     新疆|       1|\n",
      "|2004-05-07|     海南|       1|\n",
      "|2015-04-21|     辽宁|       2|\n",
      "|2005-10-20|     湖南|       1|\n",
      "|2022-04-15|     吉林|       1|\n",
      "|2017-01-31|     西藏|       1|\n",
      "|2019-12-16|     吉林|       1|\n",
      "|2002-05-12|     山西|       2|\n",
      "|2007-05-27|     安徽|       1|\n",
      "|2004-07-12|     山东|       1|\n",
      "|2019-07-07|     云南|       1|\n",
      "|2015-12-04|     四川|       1|\n",
      "|2021-07-02|     重庆|       3|\n",
      "+----------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import col,count\n",
    "from pyspark.sql import SparkSession\n",
    "# import pymysql\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JDBC Connection\").config(\"spark.driver.extraClassPath\", \"/Users/qingyanchen/Downloads/mysql-connector-java-8.0.25.jar\").getOrCreate()\n",
    "\n",
    "\n",
    "web_logs = spark.read.json('web_log.json')\n",
    "web_logs=web_logs.select(\"timestamp\",\"load_type\")\n",
    "\n",
    "web_logs2=web_logs.filter((col(\"load_type\").like(\"地方%\")))\n",
    "# web_logs2.show()\n",
    "split_col = functions.split(web_logs2['load_type'], '/')\n",
    "web_logs2=web_logs2.withColumn('load_type', split_col.getItem(1))\n",
    "#生成对时间戳切割的函数\n",
    "load_type3 = functions.split(web_logs2['timestamp'], ' ')\n",
    "#对时间戳列进行切割\n",
    "web_logs2=web_logs2.withColumn('timestamp', load_type3.getItem(0))\n",
    "web_logs2=web_logs2.groupBy(\"timestamp\",\"load_type\").agg(count(\"*\")).alias(\"count\")\n",
    "web_logs2.show()\n",
    "# Define the JDBC URL for the MySQL database\n",
    "url = \"jdbc:mysql://localhost:3306/testdata\"\n",
    "\n",
    "# Define the properties for the JDBC driver\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"123456\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "web_logs2.write.mode(\"append\").jdbc(url=url, table=\"difang_sum\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dcfeb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221.218.209.125\n",
      "202.91.128.133\n",
      "{'area': '', 'country': '中国', 'isp_id': '100026', 'queryIp': '221.218.209.125', 'city': '北京', 'ip': '221.218.209.125', 'isp': '联通', 'county': '', 'region_id': '110000', 'area_id': '', 'county_id': None, 'region': '北京', 'country_id': 'CN', 'city_id': '110100'}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://ip.taobao.com/outGetIpInfo?ip=202.91.128.133&accessKey=alibaba-inc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_ip_info(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m221.218.209.125\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_ip_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m202.91.128.133\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mget_ip_info\u001b[0;34m(ip)\u001b[0m\n\u001b[1;32m     17\u001b[0m res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://ip.taobao.com/outGetIpInfo?ip=202.91.128.133&accessKey=alibaba-inc"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@File    : ip_util.py\n",
    "@Date    : 2022-10-13\n",
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_ip_info(ip):\n",
    "    url = 'http://ip.taobao.com/outGetIpInfo'\n",
    "\n",
    "    params = {\n",
    "        'ip': ip,\n",
    "        'accessKey': 'alibaba-inc'\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, params)\n",
    "\n",
    "    if not res.ok:\n",
    "        res.raise_for_status()\n",
    "\n",
    "    return res.json().get('data')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    for i in ['221.218.209.125','202.91.128.133']:\n",
    "        print(i)\n",
    "    \n",
    "    print(get_ip_info('221.218.209.125'))\n",
    "    print(get_ip_info('202.91.128.133'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798a1e79",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m ip \u001b[38;5;241m=\u001b[39m Ip()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_IPaddress\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mIp.get_IPaddress\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_IPaddress\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mip\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m     21\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipInfo_url,data \u001b[38;5;241m=\u001b[39m data,headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m     22\u001b[0m     res \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(tmp\u001b[38;5;241m.\u001b[39mtext)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mIp.get_ip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ip\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     16\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mip_url,headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m---> 17\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mip\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Author:52loli\n",
    "# Time:2020.10.06\n",
    "import requests\n",
    "import json\n",
    " \n",
    "class Ip():\n",
    "    def __init__(self):\n",
    "        self.ip_url = 'https://www.toolnb.com/Tools/Api/IP.html'\n",
    "        self.ipInfo_url = 'https://www.toolnb.com/Tools/Api/ipgetareainfo.html'\n",
    "        self.headers = {\n",
    "            'cookie':'PHPSESSID=q8e8vhzg8nsarisdwsbmxb1g72',\n",
    "            'referer':'https://www.toolnb.com/tools/ipgetareainfo.html',\n",
    "            'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.4183.121 Safari/537.36'\n",
    "        }\n",
    "    def get_ip(self):\n",
    "        tmp = requests.post(self.ip_url,headers=self.headers)\n",
    "        result = json.loads(tmp.text)\n",
    "        return result['data']['ip']\n",
    "    def get_IPaddress(self):\n",
    "        data = {'ip': self.get_ip()}\n",
    "        tmp = requests.post(self.ipInfo_url,data = data,headers=self.headers)\n",
    "        res = json.loads(tmp.text)\n",
    "        print(res['data']['area'])\n",
    " \n",
    "ip = Ip()\n",
    "ip.get_IPaddress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "623a1d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP地址: 210.185.192.55\n",
      "国家: China\n",
      "城市: None\n",
      "纬度: 34.7732\n",
      "经度: 113.722\n"
     ]
    }
   ],
   "source": [
    "import geoip2.database\n",
    "\n",
    "# 加载GeoIP2数据库\n",
    "reader = geoip2.database.Reader('GeoLite2-City.mmdb')\n",
    "\n",
    "# 要查询的IP地址\n",
    "ip_address = '210.185.192.55'\n",
    "\n",
    "try:\n",
    "    # 查询IP地址的地理位置信息\n",
    "    response = reader.city(ip_address)\n",
    "\n",
    "    # 提取地理位置信息\n",
    "    country = response.country.name\n",
    "    city = response.city.name\n",
    "    latitude = response.location.latitude\n",
    "    longitude = response.location.longitude\n",
    "\n",
    "    # 打印结果\n",
    "    print(f'IP地址: {ip_address}')\n",
    "    print(f'国家: {country}')\n",
    "    print(f'城市: {city}')\n",
    "    print(f'纬度: {latitude}')\n",
    "    print(f'经度: {longitude}')\n",
    "\n",
    "except geoip2.errors.AddressNotFoundError:\n",
    "    print(f'找不到IP地址: {ip_address}')\n",
    "\n",
    "finally:\n",
    "    # 关闭GeoIP2数据库连接\n",
    "    reader.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a68f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
